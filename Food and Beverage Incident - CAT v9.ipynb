{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f704449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gduln001\\AppData\\Local\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nlpaug.augmenter.word as naw\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93474643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available?  False\n",
      "Number of available GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Is CUDA available? \", torch.cuda.is_available())\n",
    "print(\"Number of available GPUs:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d0bbcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Check your driver/environment setup.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Current GPU:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your driver/environment setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49baf5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11891451",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34299728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Text Cleaning and Lemmatization\n",
    "# ---------------------------------------------\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove non-informative chars\n",
    "    text = re.sub(r\"[^a-z0-9.,!?'\\s-]\", '', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # Normalize excessive punctuation\n",
    "    text = re.sub(r\"!+\", \"!\", text)\n",
    "    text = re.sub(r\"\\?+\", \"?\", text)\n",
    "    # Lemmatize tokens\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    text = \" \".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27cb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Data Loading and Preprocessing\n",
    "# ---------------------------------------------\n",
    "data = pd.read_excel('\\\\\\\\vi240c060002.woc.prod\\\\e$\\\\datasets\\\\Fields\\\\Ad Hoc Use\\\\Food_and_Beverage.xlsx')\n",
    "example_data = data.copy()\n",
    "\n",
    "text_fields = [\n",
    "    'Incident Description',\n",
    "    #'Activity Engaged in During Accident'\n",
    "    'CM Observation', \n",
    "    #'General HS Comments', \n",
    "    'Injury Description'\n",
    "]\n",
    "\n",
    "example_data[text_fields] = example_data[text_fields].fillna('')\n",
    "for field in text_fields:\n",
    "    example_data[field] = example_data[field].apply(clean_text)\n",
    "\n",
    "example_data['Combined_Text'] = (\n",
    "    example_data['Incident Description'] + ' ' +\n",
    "    #example_data['Activity Engaged in During Accident'] + ' ' +\n",
    "    example_data['CM Observation'] + ' ' +\n",
    "    #example_data['General HS Comments'] + ' ' +\n",
    "    example_data['Injury Description']\n",
    ").str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347cf30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Label Encoding\n",
    "# ---------------------------------------------\n",
    "targets = [\n",
    "    #'Event of Injury Desc', \n",
    "    #'Source of Injury Desc', \n",
    "    #'Event of Incident Desc', \n",
    "    #'Source of Incident Desc',\n",
    "    #'EDI Cause Desc'\n",
    "    'Allergy Code'\n",
    "]\n",
    "\n",
    "label_encoders = {}\n",
    "for target in targets:\n",
    "    le = LabelEncoder()\n",
    "    example_data[target + '_Encoded'] = le.fit_transform(example_data[target])\n",
    "    label_encoders[target] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979d5808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Rare Class Identification and Augmentation\n",
    "# ---------------------------------------------\n",
    "# Choose target\n",
    "focus_target = 'Allergy Code'\n",
    "focus_target_encoded = focus_target + '_Encoded'\n",
    "\n",
    "class_counts = example_data[focus_target].value_counts()\n",
    "\n",
    "# Define a rarity threshold\n",
    "rare_threshold = 50\n",
    "rare_classes_list = class_counts[class_counts < rare_threshold].index.tolist()\n",
    "\n",
    "if rare_classes_list:\n",
    "    # Augmenting rare class samples\n",
    "    syn_aug = naw.SynonymAug(aug_src='wordnet', aug_min=1, aug_max=3, aug_p=0.1)\n",
    "    \n",
    "    def augment_text(text, augmenter=syn_aug):\n",
    "        return augmenter.augment(text)\n",
    "    \n",
    "    rare_class_filter = example_data[focus_target].isin(rare_classes_list)\n",
    "    rare_class_data = example_data[rare_class_filter]\n",
    "    \n",
    "    augmented_samples = []\n",
    "    for _, row in rare_class_data.iterrows():\n",
    "        #augment once per rare sample\n",
    "        augmented_text = augment_text(row['Combined_Text'])\n",
    "        new_row = row.copy()\n",
    "        new_row['Combined_Text'] = augmented_text\n",
    "        augmented_samples.append(new_row)\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_samples)\n",
    "    example_data = pd.concat([example_data, augmented_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d76e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Tokenization and Data Split\n",
    "# ---------------------------------------------\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    example_data['Combined_Text'], \n",
    "    example_data[focus_target_encoded], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train = X_train.astype(str).tolist()\n",
    "X_test = X_test.astype(str).tolist()\n",
    "\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = Dataset(train_encodings, list(y_train))\n",
    "test_dataset = Dataset(test_encodings, list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb922fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Compute Class Weights and Implement Focal Loss\n",
    "# ---------------------------------------------\n",
    "num_labels = len(label_encoders[focus_target].classes_)\n",
    "\n",
    "class_counts_train = np.bincount(y_train)\n",
    "total_samples = len(y_train)\n",
    "class_weights = total_samples / (num_labels * class_counts_train.astype(float))\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.5, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        # Move alpha to the same device as inputs\n",
    "        if isinstance(self.alpha, torch.Tensor):\n",
    "            self.alpha = self.alpha.to(inputs.device)\n",
    "        \n",
    "        log_prob = F.log_softmax(inputs, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        \n",
    "        log_prob = log_prob.gather(dim=-1, index=targets.unsqueeze(1)).squeeze(1)\n",
    "        prob = prob.gather(dim=-1, index=targets.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        focal_weight = (1 - prob) ** self.gamma\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets] if isinstance(self.alpha, torch.Tensor) else self.alpha\n",
    "            focal_weight = focal_weight * alpha_t\n",
    "\n",
    "        loss = -focal_weight * log_prob\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "class FocalTrainer(Trainer):\n",
    "    def __init__(self, alpha=None, gamma=2.5, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fn = FocalLoss(alpha=alpha, gamma=gamma)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a630f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Compute Metrics Function\n",
    "# ---------------------------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4434a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Training Arguments and Trainer Initialization\n",
    "# ---------------------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=1250,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",    \n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    no_cuda=False  # ensures GPU is used\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "# Move model to GPU\n",
    "model.to(device)\n",
    "\n",
    "trainer = FocalTrainer(\n",
    "    alpha=class_weights,   # Incorporate class weights into focal loss\n",
    "    gamma=2.5,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad32d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Training\n",
    "# ---------------------------------------------\n",
    "trainer.train()\n",
    "model.save_pretrained('\\\\\\\\vi240c060002.woc.prod\\\\e$\\\\Machine Learning\\\\food_and_beverage_allergy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b14994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Evaluation\n",
    "# ---------------------------------------------\n",
    "eval_results = trainer.evaluate(test_dataset)\n",
    "print(\"Evaluation Results:\", eval_results)\n",
    "\n",
    "# Predict on test set for confusion matrix and classification report\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "decoded_preds = label_encoders[focus_target].inverse_transform(preds)\n",
    "decoded_labels = label_encoders[focus_target].inverse_transform(labels)\n",
    "\n",
    "cm = confusion_matrix(decoded_labels, decoded_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "report = classification_report(decoded_labels, decoded_preds, zero_division=0)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7bb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoders[focus_target].classes_, \n",
    "            yticklabels=label_encoders[focus_target].classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3153bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference example\n",
    "model = BertForSequenceClassification.from_pretrained('\\\\\\\\vi240c060002.woc.prod\\\\e$\\\\Machine Learning\\\\food_and_beverage_allergy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95003bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = [\"Nicole Lithos Clark claimed that she used the female restrooms in American Adventure and upon leaving the stall bumped the right side of her head on the coat hanger that was on the inside of the door. Felt dizzy upon walking and shopped and sat down near Japan.\"]\n",
    "new_text = [clean_text(t) for t in new_text]  # Clean the new text before prediction\n",
    "new_encodings = tokenizer(new_text, truncation=True, padding=True, max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b1284",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "outputs = model(**new_encodings)\n",
    "predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "decoded_class = label_encoders['Allergy Code'].inverse_transform([predicted_class])\n",
    "print(f\"Predicted Allergy Cause: {decoded_class[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame({\n",
    "    'Text': X_test, \n",
    "    'True_Label': label_encoders[focus_target].inverse_transform(labels),\n",
    "    'Predicted_Label': label_encoders[focus_target].inverse_transform(preds)\n",
    "})\n",
    "\n",
    "# Filter for misclassified samples\n",
    "df_misclassified = df_results[df_results['True_Label'] != df_results['Predicted_Label']]\n",
    "\n",
    "# Export to Excel\n",
    "df_misclassified.to_excel('\\\\\\\\vi240c060002.woc.prod\\\\e$\\\\data\\\\AllergyCode_misclassified_samples.xlsx', index=False)\n",
    "print(\"Misclassified samples exported to misclassified_samples.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1010ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_history = trainer.state.log_history\n",
    "\n",
    "train_epochs = []\n",
    "train_losses = []\n",
    "eval_epoch = []\n",
    "eval_losses = []\n",
    "\n",
    "for record in log_history:\n",
    "    if \"loss\" in record:\n",
    "        train_epochs.append(record[\"epoch\"])\n",
    "        train_losses.append(record[\"loss\"])\n",
    "    if \"eval_loss\" in record:\n",
    "        eval_epochs.append(record[\"epoch\"])\n",
    "        eval_losses.append(record[\"eval_loss\"])\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(train_epochs, train_losses, label='Training Loss')\n",
    "plt.plot(eval_epochs, eval_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training & Validation Loss Over Steps')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
