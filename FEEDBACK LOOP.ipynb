{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab28563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we have a feedback CSV with new or corrected labels\n",
    "feedback_df = pd.read_csv('feedback_corrections.csv')  \n",
    "# feedback_corrections.csv format: Text, Correct_Label (human provided correct class)\n",
    "\n",
    "# Encode the correct label using the same label encoder\n",
    "feedback_df[focus_target_encoded] = label_encoders[focus_target].transform(feedback_df['Correct_Label'])\n",
    "\n",
    "# Combine this feedback with your current training data\n",
    "train_feedback_combined = pd.concat([example_data, feedback_df], ignore_index=True)\n",
    "\n",
    "# Now you have a larger or corrected training set. \n",
    "# Re-run oversampling if needed\n",
    "train_feedback_combined = oversample_rare_classes(train_feedback_combined, focus_target_encoded, min_count=200)\n",
    "\n",
    "# Tokenize again\n",
    "X_train_new = train_feedback_combined['Combined_Text'].astype(str).tolist()\n",
    "y_train_new = train_feedback_combined[focus_target_encoded].values\n",
    "\n",
    "new_train_encodings = tokenizer(X_train_new, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "new_train_dataset = Dataset(new_train_encodings, list(y_train_new))\n",
    "\n",
    "# Load the previously trained model from a checkpoint\n",
    "# This reduces training time as you start from a fine-tuned model rather than from scratch.\n",
    "model_checkpoint_path = './fine_tuned_source_of_injury'  # previously saved model directory\n",
    "model = BertForSequenceClassification.from_pretrained(model_checkpoint_path, num_labels=num_labels)\n",
    "\n",
    "# Use the same training arguments but potentially fewer epochs or a smaller learning rate\n",
    "new_training_args = TrainingArguments(\n",
    "    output_dir='./results_feedback',\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=2,  # Maybe fewer epochs since we are just refining\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_feedback',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Re-initiate trainer with updated dataset\n",
    "trainer = FocalTrainer(\n",
    "    alpha=class_weights,\n",
    "    gamma=2.0,\n",
    "    model=model,\n",
    "    args=new_training_args,\n",
    "    train_dataset=new_train_dataset,\n",
    "    eval_dataset=test_dataset,  # You can keep the same test set or have a validation set here\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained('./fine_tuned_source_of_injury_feedback')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
