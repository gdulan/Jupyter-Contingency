{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f704449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nlpaug.augmenter.word as naw\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.figure_factory as ff\n",
    "import optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93474643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "#Setup environment checks\n",
    "# ---------------------------------------------\n",
    "print(\"Is CUDA available? \", torch.cuda.is_available())\n",
    "print(\"Number of available GPUs:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current GPU:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your driver/environment setup.\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11891451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "#Download NLTK data\n",
    "# ---------------------------------------------\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34299728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Text Cleaning and Lemmatization\n",
    "# ---------------------------------------------\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove non-informative chars\n",
    "    text = re.sub(r\"[^a-z0-9.,!?'\\s-]\", '', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # Normalize excessive punctuation\n",
    "    text = re.sub(r\"!+\", \"!\", text)\n",
    "    text = re.sub(r\"\\?+\", \"?\", text)\n",
    "    # Lemmatize tokens\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    text = \" \".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27cb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Data Loading and Preprocessing\n",
    "# ---------------------------------------------\n",
    "data = pd.read_excel('\\\\\\\\vi240c060002.woc.prod\\\\e$\\\\datasets\\\\Fields\\\\2ND Source_Of_Incident\\\\WCMLDataset12_23.xlsx')\n",
    "example_data = data.copy()\n",
    "\n",
    "text_fields = [\n",
    "    'Incident Description', \n",
    "    'Activity Engaged in During Accident', \n",
    "    'General HS Comments', \n",
    "    'Injury Description'\n",
    "]\n",
    "example_data[text_fields] = example_data[text_fields].fillna('')\n",
    "for field in text_fields:\n",
    "    example_data[field] = example_data[field].apply(clean_text)\n",
    "\n",
    "example_data['Combined_Text'] = (\n",
    "    example_data['Incident Description'] + ' ' +\n",
    "    example_data['Activity Engaged in During Accident'] + ' ' +\n",
    "    example_data['General HS Comments'] + ' ' +\n",
    "    example_data['Injury Description']\n",
    ").str.strip()\n",
    "\n",
    "targets = [\n",
    "    'Event of Injury Desc', \n",
    "    'Source of Injury Desc', \n",
    "    'Event of Incident Desc', \n",
    "    'Source of Incident Desc',\n",
    "    'EDI Cause Desc'\n",
    "]\n",
    "\n",
    "label_encoders = {}\n",
    "for target in targets:\n",
    "    le = LabelEncoder()\n",
    "    example_data[target + '_Encoded'] = le.fit_transform(example_data[target])\n",
    "    label_encoders[target] = le\n",
    "\n",
    "focus_target = 'Source of Incident Desc'\n",
    "focus_target_encoded = focus_target + '_Encoded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979d5808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Rare Class Identification and Augmentation\n",
    "# ---------------------------------------------\n",
    "rare_threshold = 250\n",
    "class_counts = example_data[focus_target].value_counts()\n",
    "rare_classes_list = class_counts[class_counts < rare_threshold].index.tolist()\n",
    "\n",
    "if rare_classes_list:\n",
    "    syn_aug = naw.SynonymAug(aug_src='wordnet', aug_min=1, aug_max=3, aug_p=0.1)\n",
    "    def augment_text(text, augmenter=syn_aug):\n",
    "        return augmenter.augment(text)\n",
    "    rare_class_filter = example_data[focus_target].isin(rare_classes_list)\n",
    "    rare_class_data = example_data[rare_class_filter]\n",
    "    augmented_samples = []\n",
    "    for _, row in rare_class_data.iterrows():\n",
    "        augmented_text = augment_text(row['Combined_Text'])\n",
    "        new_row = row.copy()\n",
    "        new_row['Combined_Text'] = augmented_text\n",
    "        augmented_samples.append(new_row)\n",
    "    augmented_df = pd.DataFrame(augmented_samples)\n",
    "    example_data = pd.concat([example_data, augmented_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d76e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Tokenization and Data Split\n",
    "# ---------------------------------------------\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    example_data['Combined_Text'], \n",
    "    example_data[focus_target_encoded], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train = X_train.astype(str).tolist()\n",
    "X_test = X_test.astype(str).tolist()\n",
    "\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = Dataset(train_encodings, list(y_train))\n",
    "test_dataset = Dataset(test_encodings, list(y_test))\n",
    "\n",
    "num_labels = len(label_encoders[focus_target].classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb922fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Compute Class Weights and Implement Focal Loss\n",
    "# ---------------------------------------------\n",
    "class_counts_train = np.bincount(y_train)\n",
    "total_samples = len(y_train)\n",
    "class_weights = total_samples / (num_labels * class_counts_train.astype(float))\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "    def forward(self, inputs, targets, gamma=2.0):\n",
    "        if isinstance(self.alpha, torch.Tensor):\n",
    "            self.alpha = self.alpha.to(inputs.device)\n",
    "        log_prob = F.log_softmax(inputs, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        log_prob = log_prob.gather(dim=-1, index=targets.unsqueeze(1)).squeeze(1)\n",
    "        prob = prob.gather(dim=-1, index=targets.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        focal_weight = (1 - prob) ** gamma\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]\n",
    "            focal_weight = focal_weight * alpha_t\n",
    "\n",
    "        loss = -focal_weight * log_prob\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "    \n",
    "class FocalTrainer(Trainer):\n",
    "    def __init__(self, alpha=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fn = FocalLoss(alpha=alpha)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        gamma_val = getattr(self.args, \"gamma\", 2.0)\n",
    "        loss = self.loss_fn(logits, labels, gamma=gamma_val)\n",
    "        return (loss, outputs) if return_outputs else loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a630f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Compute Metrics Function\n",
    "# ---------------------------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178bd372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_objective(metrics):\n",
    "    return metrics[\"eval_f1\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ce6d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# model_init for hyperparameter search\n",
    "# ---------------------------------------------\n",
    "def model_init(hp):\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01876a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Optuna hyperparameter search\n",
    "# ---------------------------------------------\n",
    "def hp_space_optuna(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.5, 3.5),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.001, 0.2),\n",
    "        \"warmup_steps\": trial.suggest_int(\"warmup_steps\", 300, 1500)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12921211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Trainer for hyperparameter search\n",
    "# ---------------------------------------------\n",
    "search_args = TrainingArguments(\n",
    "    output_dir='\\\\\\\\vi240c060002.woc.prod\\\\e$\\\\hyperparameters\\\\hp_search_results_sourceofincident',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=15,  \n",
    "    logging_dir='./logs_hp_search',\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=False,\n",
    "    no_cuda=(not torch.cuda.is_available())\n",
    ")\n",
    "\n",
    "search_trainer = FocalTrainer(\n",
    "    alpha=class_weights,\n",
    "    model_init=model_init,\n",
    "    args=search_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006332b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Hyperparameter Search\n",
    "# ---------------------------------------------\n",
    "best_run = search_trainer.hyperparameter_search(\n",
    "    hp_space=hp_space_optuna,\n",
    "    backend=\"optuna\",\n",
    "    n_trials=50,\n",
    "    direction=\"maximize\" \n",
    "    compute_objective=my_objective\n",
    ")\n",
    "\n",
    "print(\"Best Run:\", best_run)\n",
    "print(\"Best Hyperparams:\", best_run.hyperparameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
