{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e181ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a226f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16701b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd39f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03938507",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('\\\\\\\\vi240c060002.woc.prod\\\\e$\\\\datasets\\\\WCMLDataset12_17.xlsx')\n",
    "example_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b100101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove or normalize unwanted characters (digits, special symbols)\n",
    "    text = re.sub(r\"[^a-z0-9.,!?'\\s-]\", '', text)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Normalize excessive punctuation\n",
    "    text = re.sub(r\"!+\", \"!\", text)\n",
    "    text = re.sub(r\"\\?+\", \"?\", text)\n",
    "\n",
    "    # Tokenize by whitespace to apply lemmatization\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Lemmatize each token\n",
    "    # The WordNetLemmatizer defaults to nouns, so for a more accurate approach,\n",
    "    # you could consider POS tagging. For now, weâ€™ll assume noun form or just use the default.\n",
    "    # If you want better results, you could incorporate POS tagging:\n",
    "    # nltk.download('averaged_perceptron_tagger')\n",
    "    # then map POS tags to WordNet POS and lemmatize accordingly.\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Rejoin after lemmatization\n",
    "    text = \" \".join(tokens)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5c0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_fields = [\n",
    "    'Incident Description', \n",
    "    'Activity Engaged in During Accident', \n",
    "    'General HS Comments', \n",
    "    'Injury Description'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a223da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN with empty strings\n",
    "example_data[text_fields] = example_data[text_fields].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf949c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning function to each text field\n",
    "for field in text_fields:\n",
    "    example_data[field] = example_data[field].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b3ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text fields into a single input column (they are already lowercase from the cleaning step)\n",
    "example_data['Combined_Text'] = (\n",
    "    example_data['Incident Description'] + ' ' +\n",
    "    example_data['Activity Engaged in During Accident'] + ' ' +\n",
    "    example_data['General HS Comments'] + ' ' +\n",
    "    example_data['Injury Description']\n",
    ").str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eea8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels for all outputs\n",
    "targets = [\n",
    "    'Event of Injury Desc', \n",
    "    'Source of Injury Desc', \n",
    "    'Event of Incident Desc', \n",
    "    'Source of Incident Desc',\n",
    "    'EDI Cause Desc'\n",
    "]\n",
    "#Need "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f979ecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {}\n",
    "for target in targets:\n",
    "    le = LabelEncoder()\n",
    "    example_data[target + '_Encoded'] = le.fit_transform(example_data[target])\n",
    "    label_encoders[target] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55c72f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: split data for the \"Source of Injury Desc\" target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    example_data['Combined_Text'], \n",
    "    example_data['Source of Injury Desc_Encoded'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4390ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510c2266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58c4f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afca64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = np.bincount(y_train)  # counts how many samples of each class are in y_train\n",
    "\n",
    "num_classes = len(class_counts)\n",
    "total_samples = len(y_train)\n",
    "\n",
    "class_weights = total_samples / (num_classes * class_counts.astype(float))\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f46b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7c8ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d829d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af547c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1460e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8947edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_encodings, list(y_train))\n",
    "test_dataset = Dataset(test_encodings, list(y_test))\n",
    "\n",
    "num_labels = len(label_encoders['Source of Injury Desc'].classes_)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c492de",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f4c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of WeightedTrainer\n",
    "trainer = WeightedTrainer(\n",
    "    class_weights=class_weights,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('\\\\\\\\vi240c060002.woc.prod\\\\e$\\\\Machine Learning\\\\fine_tuned_source_of_injury')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c82455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference example\n",
    "model = BertForSequenceClassification.from_pretrained('\\\\\\\\vi240c060002.woc.prod\\\\e$\\\\Machine Learning\\\\fine_tuned_source_of_injury')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df07399",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = [\"Guest was exiting the Gran Fiesta Tour ride in Mexico Pavilion and cut her leg, Guest received a laceration on her left shin that was bleeding after exiting the Gran Fiesta Tour. Stated that she fell while trying to exit and got the laceration as she was trying to stand up. Guest stated she fell getting out of the boat at Gran Fiesta Tour at the unload platform and got a cut on her left leg. When she was walking up the exit ramp, there was BBP on the ground and she was bleeding from the cut pretty significantly as it was also on her shoes and pants.\"]\n",
    "new_text = [clean_text(t) for t in new_text]  # Clean the new text before prediction\n",
    "new_encodings = tokenizer(new_text, truncation=True, padding=True, max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eee7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "outputs = model(**new_encodings)\n",
    "predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "decoded_class = label_encoders['Source of Injury Desc'].inverse_transform([predicted_class])\n",
    "print(f\"Predicted Source of Injury: {decoded_class[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
