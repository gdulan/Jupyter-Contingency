{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a226f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nlpaug.augmenter.word as naw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c1d9789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gduln001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\gduln001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd39f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03938507",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('\\\\\\\\vi240c060002.woc.prod\\\\e$\\\\datasets\\\\WCMLDataset12_18.xlsx')\n",
    "example_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6dcdfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9a74c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove or normalize unwanted characters (digits, special symbols)\n",
    "    text = re.sub(r\"[^a-z0-9.,!?'\\s-]\", '', text)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Normalize excessive punctuation\n",
    "    text = re.sub(r\"!+\", \"!\", text)\n",
    "    text = re.sub(r\"\\?+\", \"?\", text)\n",
    "\n",
    "    # Tokenize by whitespace to apply lemmatization\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Lemmatize each token\n",
    "    # The WordNetLemmatizer defaults to nouns, so for a more accurate approach,\n",
    "    # nltk.download('averaged_perceptron_tagger')\n",
    "    # then map POS tags to WordNet POS and lemmatize accordingly.\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Rejoin after lemmatization\n",
    "    text = \" \".join(tokens)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd01182",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_fields = [\n",
    "    'Incident Description', \n",
    "    'Activity Engaged in During Accident', \n",
    "    'General HS Comments', \n",
    "    'Injury Description'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd40cf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN with empty strings\n",
    "example_data[text_fields] = example_data[text_fields].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7738f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning function to each text field\n",
    "for field in text_fields:\n",
    "    example_data[field] = example_data[field].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40d6881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text fields into a single input column (they are already lowercase from the cleaning step)\n",
    "example_data['Combined_Text'] = (\n",
    "    example_data['Incident Description'] + ' ' +\n",
    "    example_data['Activity Engaged in During Accident'] + ' ' +\n",
    "    example_data['General HS Comments'] + ' ' +\n",
    "    example_data['Injury Description']\n",
    ").str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebc8f1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gduln001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "syn_aug = naw.SynonymAug(aug_src='wordnet', aug_min=1, aug_max=3, aug_p=0.1)\n",
    "\n",
    "def augment_text(text, augmenter=syn_aug):\n",
    "    return augmenter.augment(text)\n",
    "\n",
    "# Apply augmentation to rare class samples\n",
    "rare_threshold = 50\n",
    "\n",
    "# Compute the frequency of each class\n",
    "class_counts = example_data['Source of Injury Desc'].value_counts()\n",
    "\n",
    "# Identify which classes are rare\n",
    "rare_classes_list = class_counts[class_counts < rare_threshold].index.tolist()\n",
    "\n",
    "# Now this variable is defined, you can filter the dataframe\n",
    "rare_class_filter = example_data['Source of Injury Desc'].isin(rare_classes_list)\n",
    "rare_class_data = example_data[rare_class_filter]\n",
    "\n",
    "augmented_samples = []\n",
    "for _, row in rare_class_data.iterrows():\n",
    "    augmented_text = augment_text(row['Combined_Text'])\n",
    "    new_row = row.copy()\n",
    "    new_row['Combined_Text'] = augmented_text\n",
    "    augmented_samples.append(new_row)\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_samples)\n",
    "example_data = pd.concat([example_data, augmented_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f324cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels for all outputs\n",
    "targets = [\n",
    "    'Event of Injury Desc', \n",
    "    'Source of Injury Desc', \n",
    "    'Event of Incident Desc', \n",
    "    'Source of Incident Desc',\n",
    "    'EDI Cause Desc'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0740558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {}\n",
    "for target in targets:\n",
    "    le = LabelEncoder()\n",
    "    example_data[target + '_Encoded'] = le.fit_transform(example_data[target])\n",
    "    label_encoders[target] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314d5830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4edae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data for the \"Source of Injury Desc\" target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    example_data['Combined_Text'], \n",
    "    example_data['Source of Injury Desc_Encoded'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f38de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e8d264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c838a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9765c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a6bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = np.bincount(y_train)  # counts how many samples of each class are in y_train\n",
    "\n",
    "num_classes = len(class_counts)\n",
    "total_samples = len(y_train)\n",
    "\n",
    "class_weights = total_samples / (num_classes * class_counts.astype(float))\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283759b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a646d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f28299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e28c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_encodings, list(y_train))\n",
    "test_dataset = Dataset(test_encodings, list(y_test))\n",
    "\n",
    "num_labels = len(label_encoders['Source of Injury Desc'].classes_)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43c3149",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c389c416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of WeightedTrainer\n",
    "trainer = WeightedTrainer(\n",
    "    class_weights=class_weights,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db24f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('\\\\\\\\vi240c060002.woc.prod\\\\e$\\\\Machine Learning\\\\fine_tuned_source_of_injury')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfd1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference example\n",
    "model = BertForSequenceClassification.from_pretrained('\\\\\\\\vi240c060002.woc.prod\\\\e$\\\\Machine Learning\\\\fine_tuned_source_of_injury')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f192537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = [\"food borne illness FSH Incident per guest email dated 12/08/24 ppw to be sent. tmg. Guest states that 3 guests in their party became sick with food poisoning symptoms after eating at the buffet and one of them needed medical attention for the symptoms after returning home.\"]\n",
    "new_text = [clean_text(t) for t in new_text]  # Clean the new text before prediction\n",
    "new_encodings = tokenizer(new_text, truncation=True, padding=True, max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364be6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "outputs = model(**new_encodings)\n",
    "predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "decoded_class = label_encoders['Source of Injury Desc'].inverse_transform([predicted_class])\n",
    "print(f\"Predicted Source of Injury: {decoded_class[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81de0610",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoders['Source of Injury Desc'].classes_, yticklabels=label_encoders['Source of Injury Desc'].classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8568c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from the model on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Inverse transform labels (Human Readable)\n",
    "decoded_preds = label_encoders['Source of Injury Desc'].inverse_transform(preds)\n",
    "decoded_labels = label_encoders['Source of Injury Desc'].inverse_transform(labels)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(decoded_labels, decoded_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(decoded_labels, decoded_preds, zero_division=0)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
