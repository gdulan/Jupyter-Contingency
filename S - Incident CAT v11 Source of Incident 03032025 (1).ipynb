{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f704449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gduln001\\AppData\\Local\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nlpaug.augmenter.word as naw\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.figure_factory as ff\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2102bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is CUDA available? \", torch.cuda.is_available())\n",
    "print(\"Number of available GPUs:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd2a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Current GPU:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your driver/environment setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa63f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11891451",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34299728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Text Cleaning and Lemmatization\n",
    "# ---------------------------------------------\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove non-informative chars\n",
    "    text = re.sub(r\"[^a-z0-9.,!?'\\s-]\", '', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # Normalize excessive punctuation\n",
    "    text = re.sub(r\"!+\", \"!\", text)\n",
    "    text = re.sub(r\"\\?+\", \"?\", text)\n",
    "    # Lemmatize tokens\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    text = \" \".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27cb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Data Loading and Preprocessing\n",
    "# ---------------------------------------------\n",
    "data = pd.read_excel('\\\\\\\\vi240c060002.woc.prod\\\\e$\\\\datasets\\\\Fields\\\\2ND Source_Of_Incident\\\\WCMLDataset12_23.xlsx')\n",
    "example_data = data.copy()\n",
    "\n",
    "text_fields = [\n",
    "    'Incident Description', \n",
    "    'Activity Engaged in During Accident', \n",
    "    'General HS Comments', \n",
    "    'Injury Description'\n",
    "]\n",
    "\n",
    "example_data[text_fields] = example_data[text_fields].fillna('')\n",
    "for field in text_fields:\n",
    "    example_data[field] = example_data[field].apply(clean_text)\n",
    "\n",
    "example_data['Combined_Text'] = (\n",
    "    example_data['Incident Description'] + ' ' +\n",
    "    example_data['Activity Engaged in During Accident'] + ' ' +\n",
    "    example_data['General HS Comments'] + ' ' +\n",
    "    example_data['Injury Description']\n",
    ").str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347cf30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Label Encoding\n",
    "# ---------------------------------------------\n",
    "targets = [\n",
    "    'Event of Injury Desc', \n",
    "    'Source of Injury Desc', \n",
    "    'Event of Incident Desc', \n",
    "    'Source of Incident Desc',\n",
    "    'EDI Cause Desc'\n",
    "]\n",
    "\n",
    "label_encoders = {}\n",
    "for target in targets:\n",
    "    le = LabelEncoder()\n",
    "    example_data[target + '_Encoded'] = le.fit_transform(example_data[target])\n",
    "    label_encoders[target] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979d5808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Rare Class Identification and Augmentation\n",
    "# ---------------------------------------------\n",
    "# Choose target\n",
    "focus_target = 'Source of Incident Desc'\n",
    "focus_target_encoded = focus_target + '_Encoded'\n",
    "\n",
    "class_counts = example_data[focus_target].value_counts()\n",
    "\n",
    "# Define a rarity threshold\n",
    "rare_threshold = 50\n",
    "rare_classes_list = class_counts[class_counts < rare_threshold].index.tolist()\n",
    "\n",
    "if rare_classes_list:\n",
    "    # Augmenting rare class samples\n",
    "    syn_aug = naw.SynonymAug(aug_src='wordnet', aug_min=1, aug_max=3, aug_p=0.1)\n",
    "    \n",
    "    def augment_text(text, augmenter=syn_aug):\n",
    "        return augmenter.augment(text)\n",
    "    \n",
    "    rare_class_filter = example_data[focus_target].isin(rare_classes_list)\n",
    "    rare_class_data = example_data[rare_class_filter]\n",
    "    \n",
    "    augmented_samples = []\n",
    "    for _, row in rare_class_data.iterrows():\n",
    "        #augment once per rare sample\n",
    "        augmented_text = augment_text(row['Combined_Text'])\n",
    "        new_row = row.copy()\n",
    "        new_row['Combined_Text'] = augmented_text\n",
    "        augmented_samples.append(new_row)\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_samples)\n",
    "    example_data = pd.concat([example_data, augmented_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d76e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Tokenization and Data Split\n",
    "# ---------------------------------------------\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    example_data['Combined_Text'], \n",
    "    example_data[focus_target_encoded], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train = X_train.astype(str).tolist()\n",
    "X_test = X_test.astype(str).tolist()\n",
    "\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = Dataset(train_encodings, list(y_train))\n",
    "test_dataset = Dataset(test_encodings, list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb922fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Compute Class Weights and Implement Focal Loss\n",
    "# ---------------------------------------------\n",
    "num_labels = len(label_encoders[focus_target].classes_)\n",
    "\n",
    "class_counts_train = np.bincount(y_train)\n",
    "total_samples = len(y_train)\n",
    "class_weights = total_samples / (num_labels * class_counts_train.astype(float))\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.9752029865067646, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        # Move alpha to the same device as inputs\n",
    "        if isinstance(self.alpha, torch.Tensor):\n",
    "            self.alpha = self.alpha.to(inputs.device)\n",
    "        \n",
    "        log_prob = F.log_softmax(inputs, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        \n",
    "        log_prob = log_prob.gather(dim=-1, index=targets.unsqueeze(1)).squeeze(1)\n",
    "        prob = prob.gather(dim=-1, index=targets.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        focal_weight = (1 - prob) ** self.gamma\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets] if isinstance(self.alpha, torch.Tensor) else self.alpha\n",
    "            focal_weight = focal_weight * alpha_t\n",
    "\n",
    "        loss = -focal_weight * log_prob\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "class FocalTrainer(Trainer):\n",
    "    def __init__(self, alpha=None, gamma=2.9752029865067646, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fn = FocalLoss(alpha=alpha, gamma=gamma)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a630f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Compute Metrics Function\n",
    "# ---------------------------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4434a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Training Arguments and Trainer Initialization\n",
    "# ---------------------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2.63395e-05,\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=1311,\n",
    "    weight_decay=0.17765438394725372,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",    \n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    no_cuda=False  # ensures GPU is used\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "# Move model to GPU\n",
    "model.to(device)\n",
    "\n",
    "trainer = FocalTrainer(\n",
    "    alpha=class_weights,   # Incorporate class weights into focal loss\n",
    "    gamma=2.9752029865067646,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad32d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Training\n",
    "# ---------------------------------------------\n",
    "trainer.train()\n",
    "model = trainer.model\n",
    "model.save_pretrained('\\\\\\\\vi240c060002.woc.prod\\\\e$\\\\Machine Learning\\\\3_3_source_of_incident')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b14994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Evaluation\n",
    "# ---------------------------------------------\n",
    "eval_results = trainer.evaluate(test_dataset)\n",
    "print(\"Evaluation Results:\", eval_results)\n",
    "\n",
    "# Predict on test set for confusion matrix and classification report\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "decoded_preds = label_encoders[focus_target].inverse_transform(preds)\n",
    "decoded_labels = label_encoders[focus_target].inverse_transform(labels)\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, linewidth=200)\n",
    "cm = confusion_matrix(decoded_labels, decoded_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "report = classification_report(decoded_labels, decoded_preds, zero_division=0)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7bb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize confusion matrix\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoders[focus_target].classes_, \n",
    "            yticklabels=label_encoders[focus_target].classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3153bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference example\n",
    "model = BertForSequenceClassification.from_pretrained('\\\\\\\\vi240c060002.woc.prod\\\\e$\\\\Machine Learning\\\\\\\\Current Production Backups\\\\3_3_source_of_incident')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95003bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = [\"BI set up per guest call 12/10/24 ZP. Gst hit head on the floor after a sharp turn made by the bus. Previous Comments. Guest states that they were secured in the QPod in their personal wheelchair. Bus 5374 made a left turn, during which the rear right wheel of the wheelchair bent in half, causing the Guest to tip over, making contact with the right side of his head to a vertical handrail near the rear door of the bus. Guest stated that they were not in need of immediate medical attention and declined offers, but later called their resort FD from the room requesting transport to an urgent care facility. Guest states that the right side of his head made contact with a vertical handrail near the rear door of the bus.Guest was taken to Advent health and then transported to emergency room at celebration where a CT scan was done, no brain bleed. Gst was provided refunds for tickets and resort, however he requested monetary compensation for the time lost and for the hospital bills. 12/10/24 ZP Previous Comments. Cast Member states that she heard a noise from the rear of the bus as she was making her turn and looked in her mirror to see that the Guest was leaning over and was notified by members of the Guest's party that he had hit his head. Guest declined any offers of medical attention at the time. CM also noted that the chair was still secured with all three straps in place\"]\n",
    "new_text = [clean_text(t) for t in new_text]  # Clean the new text before prediction\n",
    "new_encodings = tokenizer(new_text, truncation=True, padding=True, max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b1284",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "outputs = model(**new_encodings)\n",
    "predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "decoded_class = label_encoders['Source of Incident Desc'].inverse_transform([predicted_class])\n",
    "print(f\"Predicted Source of Incident: {decoded_class[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame({\n",
    "    'Text': X_test, \n",
    "    'True_Label': label_encoders[focus_target].inverse_transform(labels),\n",
    "    'Predicted_Label': label_encoders[focus_target].inverse_transform(preds)\n",
    "})\n",
    "\n",
    "# Filter for misclassified samples\n",
    "df_misclassified = df_results[df_results['True_Label'] != df_results['Predicted_Label']]\n",
    "\n",
    "# Export to Excel\n",
    "df_misclassified.to_excel('\\\\\\\\vi240c060002.woc.prod\\\\e$\\\\data\\\\misclassified_samples.xlsx', index=False)\n",
    "print(\"Misclassified samples exported to misclassified_samples.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1010ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_history = trainer.state.log_history\n",
    " \n",
    "train_epochs = []\n",
    "train_losses = []\n",
    "eval_epoch = []\n",
    "eval_losses = []\n",
    " \n",
    "for record in log_history:\n",
    "    if \"loss\" in record:\n",
    "        train_epochs.append(record[\"epoch\"])\n",
    "        train_losses.append(record[\"loss\"])\n",
    "    if \"eval_loss\" in record:\n",
    "        eval_epoch.append(record[\"epoch\"])\n",
    "        eval_losses.append(record[\"eval_loss\"])\n",
    " \n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(train_epochs, train_losses, label='Training Loss')\n",
    "plt.plot(eval_epoch, eval_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training & Validation Loss Over Steps')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
